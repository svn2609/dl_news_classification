# ðŸ“° AG News Classification with LoRA + RoBERTa

This repository contains the code and notebook for a deep learning project that fine-tunes the `roberta-base` model using [LoRA (Low-Rank Adaptation)](https://arxiv.org/abs/2106.09685) on the [AG News dataset](https://huggingface.co/datasets/ag_news). The project explores parameter-efficient fine-tuning, contextual data augmentation, and mixed-precision (FP16) training to achieve state-of-the-art performance with minimal resource usage.

---

## ðŸš€ Project Highlights

- ðŸ“š **Model**: RoBERTa-base with LoRA adapters (trainable params < 1M)
- ðŸ§  **Technique**: Parameter-Efficient Fine-Tuning using LoRA
- ðŸ” **Augmentation**: Contextual word replacement using MLM (RoBERTa)
- âš™ï¸ **Training**: Mixed precision FP16 with Hugging Face Trainer
- ðŸ“ˆ **Accuracy**: ~95.3% on AG News test set with macro F1 parity
- ðŸ’¾ **Efficient**: Trainable parameters â‰ˆ 0.6% of full model

---

## ðŸ“‚ Repository Structure

```bash
â”œâ”€â”€ Roberta_news_classification.ipynb     # Jupyter notebook with full implementation
â”œâ”€â”€ accuracy-vs-epoch.png         
â”œâ”€â”€ loss-vs-epoch.png             
â”œâ”€â”€ main.tex                     
â””â”€â”€ README.md
â”œâ”€â”€ inference_output.csv                   
â””â”€â”€ test_unlabelled.pkl
